:doctype: book
:imagesdir: images
:stylesheet: paper.css
:last-update-label!:
:source-highlighter: highlight.js
:highlightjs-theme: thesis
:highlightjsdir: libraries/highlightjs
:stem:
:toc: macro
:xrefstyle: short
ifndef::env-vscode[]
:kroki-fetch-diagram: true
:kroki-default-options: inline
endif::env-vscode[]
:cpp: C++

image::../assets/logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete#main-title]
= Preliminary lab report for the third high-performance computing exercise

[.presented-by.text-center]
by +
****REMOVED**** ***REMOVED*** +
****REMOVED**** ***REMOVED***

:part-signifier: Part
:listing-caption: Listing

== Intro 

In the third lab we will try to run a distributed sorting algorithm using openMPI. We are using Rust and MPI to implement the third lab.

=== Non-distributed implementation

We wrote a non-distributed implementation and compared it to the reference CPP implementation. We tried various sorting algorithms and found that radix sort is the fastest, but the standard librarys `sort_unstable()` is not far behind.

<<single_threaded_implementation>> shows the sequential implementation of the matrix multiplication. The algorithm is pretty straight forward. We iterate over the rows of the first matrix and the columns of the second matrix. For each element we calculate the dot product of the row and column. The result is stored in the `result` matrix.

=== Distributed approach

Our distributed implementation is based on dividing the data into buckets and streaming the buckets to the other processes. We use a clear manager/worker approach. The manager is responsible for dividing the data into buckets and sending them to the workers. The workers receive the buckets and sort them. The manager then receives the sorted buckets and merges them into a sorted list.

<<server-flow>> and <<client-flow>> show the flow of the manager and the workers.

// .Reading and distributing the data
// [nomnoml]
// ....
// [<start> start]->
// [Read block of unsorted data]->[Split into 256 buckets;based on the first byte]->
// [Send each bucket to responsible worker; _;
// worker = firstByte % numberOfWorkers]->
// [<choice> Unsorted data left?]yes->[Read 1mb of unsorted data]
// [<choice> Unsorted data left?]no ->[Send done to each worker]->
// [<end> server]--[<note> All data has been distributed to all workers]

// [<start> start client]->
// [Create 256 empty lists]->
// [Receive message]->[<choice> Is done message?]no ->
// [Sort received data]->[Merge into the list with the same prefix]->[Receive message]

// [<choice> Is done message?]yes ->
// [Discard empty lists]->
// [<end> client]--[<note> We now have a sorted list for;
// all buckets this worker is responsible for]
// ....

// .Receiving the data
// [nomnoml]
// ....
// [<start> start client]->
// [Discard empty buckets]->[Send next bucket to server]->[<choice> Buckets left?]yes ->[Send next bucket to server]
// [<choice> Buckets left?]no ->
// [Send done message]

// [<start> start server]->
// [Create 256 empty lists]->
// [Create list with workers that are not finished]->
// [Receive message from any worker]->[<choice> Is done message?]no ->
// [Append data to the correct list] ->[Receive message from any worker]


// [<choice> Is done message?] yes ->[Mark worker as finished] ->
// [<choice> Are all workers finished?]no -> [Receive message from any worker]
// [<choice> Are all workers finished?]yes ->[Concatenate the 256 sorted lists into one sorted list]-> [Write the sorted list into a file]
// ....


.Server flow
[nomnoml#server-flow,opts=inline,width=22cm]
....
#.box: fill=#8f8 dashed


[<start> start]->
[Read block of unsorted data]->[Split into 256 buckets;based on the first byte]->
[Send each bucket to responsible worker; _;
worker = firstByte % numberOfWorkers]->
[<choice> Unsorted data left?]yes->[Read block of unsorted data]
[<choice> Unsorted data left?]no ->[Send done to each worker]->
[<box> Finished reading]->
[Create 256 empty lists]->
[Receive message from any worker]->[<choice> Is done message?]yes ->[<choice> Are all workers done]
[<choice> Is done message?]no ->
[Insert data into the correct list based on its prefix] ->[Receive message from any worker]


[<choice> Are all workers done]no -> [Receive message from any worker]
[<choice> Are all workers done]yes ->[Concatenate the 256 sorted lists and write them in a file]->
[<end> end]
....

.Client flow
[nomnoml#client-flow]
....
#.box: fill=#8f8 dashed

[<start> start client]->
[Create 256 empty lists]->
[Receive message]->[<choice> Is done message?]no ->
[Sort received data]->[Merge into the list with the same prefix]->[Receive message]

[<choice> Is done message?]yes ->
[<box> Finished reading]->
[Discard empty buckets]->[Send next bucket to server]->[<choice> Buckets left?]yes ->[Send next bucket to server]
[<choice> Buckets left?]no ->
[Send done message]->
[<end> client]
....

=== Measurements

// We did not have time to measure performance at the Virgo cluster. We did measure the performance on our local machines. We used a 10GB file with generated with gensort. The single node implemetation is the non-mpi implementation. The measurements for more than one node are the MPI implementation.
// <<sorting-data>> show that our MPI implementation is always slower than the single threaded implementation. This is probably due to the overhead of sending the data over the network. We could probably optimize our implementation to be better, but we did not have time to do so.

We measured performance for datasets sized between 2^10 to 2^22 bytes on the Virgo cluster. These datasets were sorted by 4 algorithms:
`radix-sort`:: A single threaded radix sort implementation.
`unstable_sort`:: The rust standard librarys `sort_unstable()` https://doc.rust-lang.org/std/primitive.slice.html#method.sort_unstable function which uses pattern-defeating quicksort. 
`mpi-single`:: Our MPI implementation with a single thread per node.
`mpi-multi`:: Our MPI implementation with 2-4 threades per node, so reading, transmitting, sorting, and writing can happen in parallel.

We tested the MPI implementations with 1 to 16 nodes and 1 to 16 tasks per node (but at least 2 tasks). The two non-MPI implementations were tested with one node/one task. First the input file is copied to the `/tmp` directory on the manager node. The sorting algrithm then uses that file as input to avoid lustre bottlenecks. The output file is also written to the `/tmp` directory on the manager node. The measurement does not include coping the input file to the `/tmp` directory or copying the output file from the `/tmp` directory. The measurement does include reading and writing of the files from the `/tmp` directory. All measurements were made from inside our application. We also measured individual times for reading, transmitting, sorting, and writing to indentify bottlenecks. We tried to measure each implementation for each applicable combination of nodes and tasks per node 16 times. We bundled four measurements for the same configuration into one slurm batch.  In reality we got fewer measurements, as they sometimes fail. 

=== Non distributed performance

We want to select a non-distributed implementation to use as a baseline when comparing the distributed implementations. <<non-distributed-performance-total>> shows the total runtime of the two tested non-distributed implementations for different dataset sizes. It shows that the `sort-unstable` implementation is faster than the `radix-sort` implementation for smaller datasets. It's measurements are also more predictable, as the curve does not have a weird bump at 2^18 bytes. <<non-distributed-performance-per-step>> shows that the bump is probably not a measurement error, as the difference is in the sorting step and not while reading the input or writing the result.

.Total runtime comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-total
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-total.vl.json
include::scripts/vega-chart.adoc[]

.Runtime breakdown comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-per-step
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-per-step.vl.json
include::scripts/vega-chart.adoc[]

We will use the `sort-unstable` implementation as a baseline for the distributed implementations, because it is faster and has a more predictable runtime.

=== Distributed performance


=== Learnings

Rust MPI is ok, but not great. It is not very ergonomic as it mostly just wraps C calls. The documentation is acceptable. One of the major pitfalls we encountered was with the `receive_vec` function which is a wrapper around `MPI_recv`. It puts the data directly in a heap allocated vector. It is quite slow for large buffers, as it does not know the size of the data it is receiving in advance, so it has to grow the Vec repeatedly. We ended up using `receive_into` a buffer instead, which is a bit more verbose, but much faster.

We also learned that MPI has a limit of 2GB per buffer, which can be problematic when working with large datasets, like 100GB of sorting data.

The posix fadvise API is cool and can be used to tell the OS that we are going to read a file sequentially and only once. This can improve performance by a tiny bit, as the kernel can prefetch more the data. Memadvise is probably also cool, but we did not have time to try it out.

Lustre can be quite slow, probably due to congestion.

=== Benchmarking protocol

We want to benchmark our algorithm for the 1G entries dataset. Our benchmark has to read the data from the /tmp partition from a single node and write the result back to the /tmp partition on a single node. We do this, because we want to benchmark our algorithm (and the performance of the nodes and the networking performance). Sorting should happen in a distributed fashion.

First, we will copy the data to the tmp partition of the root node to avoid the pitfalls of using lustre. THen...

.Our MPI implementation
:chart-id: id=sorting-data
:vega-lite-filename: processed-assets/sorting-data.vl.json
include::scripts/vega-chart.adoc[]



include::scripts/trailing-scripts.adoc[]