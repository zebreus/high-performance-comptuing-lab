:doctype: book
:imagesdir: images
:stylesheet: paper.css
:last-update-label!:
:source-highlighter: highlight.js
:highlightjs-theme: thesis
:highlightjsdir: libraries/highlightjs
:stem:
:toc: macro
:xrefstyle: short
ifndef::env-vscode[]
:kroki-fetch-diagram: true
:kroki-default-options: inline
endif::env-vscode[]
:cpp: C++

image::../assets/logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete#main-title]
= Preliminary lab report for the fourth high-performance computing exercise

[.presented-by.text-center]
by +
****REMOVED**** ***REMOVED*** +
****REMOVED**** ***REMOVED***

:part-signifier: Part
:listing-caption: Listing

== Intro 

In the fourth lab we will build a lattice gas cellular automata (LGCA) using Rust, Rayon and MPI.

=== Concept

Our LGCA will use the FHP-I model. It uses a hexagonal grid and does not support resting particles.

==== Requirements

The following requirements are given by the lab instructions:

* Essentially, the lattice gas consists of a hexagonal array (the lattice), and a set of barriers.
* Each cell in the lattice is connected to 6 neighbors (see the illustrations in the report refereced above).
* Time is discrete (i.e. t = 0, 1, 2, 3...).
* Each connection is boolean: Thus, at time t, there is either 0 or 1 particle (but never 2 or more particles) traveling from any given cell a to any cellb, if a and b are neighbors (there might also be a particle traveling from b to a; the two links are independent of each other).
* The state of the lattice at time = t can be calculated in two phases:
* Transport: First, the particles travel linearly. For example, if cells a, b and c are linear neighbors (e.g. c is to the right of b, and b is to the right of a), then a particle traveling from a to b at time = 1 will continue to travel to c at time = 2 (in the absence of collisions, see below).
* Scattering: Second, collisions with other particles and with the barriers are taken into account. Collisions between particles are non-deterministic: More than one outcome is (sometimes) possible, and the simulation chooses between these outcomes randomly. See page 14 (188) in [Has87] for a table of the possible collisions.
* Particles reflect off barriers.



=== Implementation

Our hex grid will use even-row offset coordinates. It is divided into multiple "rectangular" sections that are distributed to different nodes. Each node will run use multithreading to process the movements of particles in its sections and then send the particles that left its section to the node responsible for that section. Then each node will process the collisions of the particles in its section. Repeat until the simulation is done.

=== Results

Our results will focus on the following three questions from the exercise sheet:

* Measure the performance in terms of the number of cells X time steps which can be calculated per minute.
* Determine the empirical relationship between the number of processors and the performance, and between the size of the lattice and the performance.
* What is the largest lattice which you can simulate?

.Server flow
[nomnoml#server-flow,opts=inline,width=22cm]
....
#.box: fill=#8f8 dashed


[<start> start]->
[Read block of unsorted data]->[Split into 256 buckets;based on the first byte]->
[Send each bucket to responsible worker; _;
worker = firstByte % numberOfWorkers]->
[<choice> Unsorted data left?]yes->[Read block of unsorted data]
[<choice> Unsorted data left?]no ->[Send done to each worker]->
[<box> Finished reading]->
[Create 256 empty lists]->
[Receive message from any worker]->[<choice> Is done message?]yes ->[<choice> Are all workers done]
[<choice> Is done message?]no ->
[Insert data into the correct list based on its prefix] ->[Receive message from any worker]


[<choice> Are all workers done]no -> [Receive message from any worker]
[<choice> Are all workers done]yes ->[Concatenate the 256 sorted lists and write them in a file]->
[<end> end]
....

.Client flow
[nomnoml#client-flow]
....
#.box: fill=#8f8 dashed

[<start> start client]->
[Create 256 empty lists]->
[Receive message]->[<choice> Is done message?]no ->
[Sort received data]->[Merge into the list with the same prefix]->[Receive message]

[<choice> Is done message?]yes ->
[<box> Finished reading]->
[Discard empty buckets]->[Send next bucket to server]->[<choice> Buckets left?]yes ->[Send next bucket to server]
[<choice> Buckets left?]no ->
[Send done message]->
[<end> client]
....

=== Measurements

// We did not have time to measure performance at the Virgo cluster. We did measure the performance on our local machines. We used a 10GB file with generated with gensort. The single node implemetation is the non-mpi implementation. The measurements for more than one node are the MPI implementation.
// <<sorting-data>> show that our MPI implementation is always slower than the single threaded implementation. This is probably due to the overhead of sending the data over the network. We could probably optimize our implementation to be better, but we did not have time to do so.

We measured performance for datasets sized between 2^10 to 2^22 bytes on the Virgo cluster. These datasets were sorted by 4 algorithms:
`radix-sort`:: A single threaded radix sort implementation.
`unstable_sort`:: The rust standard librarys `sort_unstable()` https://doc.rust-lang.org/std/primitive.slice.html#method.sort_unstable function which uses pattern-defeating quicksort. 
`mpi-single`:: Our MPI implementation with a single thread per node.
`mpi-multi`:: Our MPI implementation with 2-4 threades per node, so reading, transmitting, sorting, and writing can happen in parallel.

We tested the MPI implementations with 1 to 16 nodes and 1 to 16 tasks per node (but at least 2 tasks). The two non-MPI implementations were tested with one node/one task. First the input file is copied to the `/tmp` directory on the manager node. The sorting algrithm then uses that file as input to avoid lustre bottlenecks. The output file is also written to the `/tmp` directory on the manager node. The measurement does not include coping the input file to the `/tmp` directory or copying the output file from the `/tmp` directory. The measurement does include reading and writing of the files from the `/tmp` directory. All measurements were made from inside our application. We also measured individual times for reading, transmitting, sorting, and writing to indentify bottlenecks. We tried to measure each implementation for each applicable combination of nodes and tasks per node 16 times. We bundled four measurements for the same configuration into one slurm batch.  In reality we got fewer measurements, as they sometimes fail. 

=== Non distributed performance

We want to select a non-distributed implementation to use as a baseline when comparing the distributed implementations. <<non-distributed-performance-total>> shows the total runtime of the two tested non-distributed implementations for different dataset sizes. It shows that the `sort-unstable` implementation is faster than the `radix-sort` implementation for smaller datasets. It's measurements are also more predictable, as the curve does not have a weird bump at 2^18 bytes. <<non-distributed-performance-per-step>> shows that the bump is probably not a measurement error, as the difference is in the sorting step and not while reading the input or writing the result.

.Total runtime comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-total
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-total.vl.json
include::scripts/vega-chart.adoc[]

.Runtime breakdown comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-per-step
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-per-step.vl.json
include::scripts/vega-chart.adoc[]

.Relative step comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-percentage
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-percentage.vl.json
include::scripts/vega-chart.adoc[]

We will use the `sort-unstable` implementation as a baseline for the distributed implementations, because it is faster and has a more predictable runtime.

=== Distributed performance

<<mpi-single-performance-one-node-speedup>> shows the relative speedup of the distributed implementation with different numbers of tasks compared to `sort-unstable`. Each line represents a implementation running with a given number of ranks. Every rank runs on the same machine.

.Speedup compared to non-distributed sorting
:chart-id: id=mpi-single-performance-one-node-speedup
:vega-lite-filename: processed-assets/sorting-mpi-single-performance-one-node-speedup.vl.json
include::scripts/vega-chart.adoc[]

.Efficiency compared to non-distributed sorting
:chart-id: id=mpi-single-performance-one-node-efficiency
:vega-lite-filename: processed-assets/sorting-mpi-single-performance-one-node-efficiency.vl.json
include::scripts/vega-chart.adoc[]

It shows that the distributed implementation is slower than the non-distributed implementation for small problems (n <= 2^20 entries). The best relative speedup is achieved for problems with a size of 2^24 , after that the relative speedup gets lower again.

When only two tasks are used, there is no speedup. With 2^24 entries the speedup is exactly 1, which means that the distributed implementation is as fast as the non-distributed implementation. I would have expected it to be lower, as we only have one worker that is actually sorting in that case. The advantage we get by bucketing the data and only sorting small chunks, seems to be equal to the overhead of distributing it into buckets and sending the data over the network.
// Wrong... It is only half that..

When using 4 or 8 tasks the speedup is 2 for 2^24 entries. For 16, 32, and 64 tasks the speedup is 3 for 2^24 entries. It looks like the maximum speedup is achieved when using 16 tasks, after that we get diminishing returns.


.Duration of each step when sorting 16M numbers
:chart-id: id=one-node-16M-steps
:vega-lite-filename: processed-assets/sorting-one-node-16M-steps.vl.json
include::scripts/vega-chart.adoc[]


=== Learnings

Rust MPI is ok, but not great. It is not very ergonomic as it mostly just wraps C calls. The documentation is acceptable. One of the major pitfalls we encountered was with the `receive_vec` function which is a wrapper around `MPI_recv`. It puts the data directly in a heap allocated vector. It is quite slow for large buffers, as it does not know the size of the data it is receiving in advance, so it has to grow the Vec repeatedly. We ended up using `receive_into` a buffer instead, which is a bit more verbose, but much faster.

We also learned that MPI has a limit of 2GB per buffer, which can be problematic when working with large datasets, like 100GB of sorting data.

The posix fadvise API is cool and can be used to tell the OS that we are going to read a file sequentially and only once. This can improve performance by a tiny bit, as the kernel can prefetch more the data. Memadvise is probably also cool, but we did not have time to try it out.

Lustre can be quite slow, probably due to congestion.

=== Benchmarking protocol

We want to benchmark our algorithm for the 1G entries dataset. Our benchmark has to read the data from the /tmp partition from a single node and write the result back to the /tmp partition on a single node. We do this, because we want to benchmark our algorithm (and the performance of the nodes and the networking performance). Sorting should happen in a distributed fashion.

First, we will copy the data to the tmp partition of the root node to avoid the pitfalls of using lustre. THen...

.Our MPI implementation
:chart-id: id=sorting-data
:vega-lite-filename: processed-assets/sorting-data.vl.json
include::scripts/vega-chart.adoc[]

[glossary]
== List of abbreviations
// Abbreviations from here will automatically be linked to the document

// Abbreviations in random order and links to read more about them
[glossary]
[[LGCA]]LGCA:: Lattice gas cellular automaton link:pass:[https://en.wikipedia.org/wiki/Lattice_gas_automaton][🔗^]


include::scripts/trailing-scripts.adoc[]