:doctype: book
:imagesdir: images
:stylesheet: paper.css
:last-update-label!:
:source-highlighter: highlight.js
:highlightjs-theme: thesis
:highlightjsdir: libraries/highlightjs
:stem:
:toc: macro
:xrefstyle: short
ifndef::env-vscode[]
:kroki-fetch-diagram: true
:kroki-default-options: inline
endif::env-vscode[]
:cpp: C++

image::../assets/logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete#main-title]
= Preliminary lab report for the fourth high-performance computing exercise

[.presented-by.text-center]
by +
****REMOVED**** ***REMOVED*** +
****REMOVED**** ***REMOVED***

:part-signifier: Part
:listing-caption: Listing

== Intro 

In the fourth lab we will build a lattice gas cellular automata (LGCA) using Rust, Rayon and MPI.

=== Concept

Our LGCA will use the FHP-I model. It uses a hexagonal grid and does not support resting particles.

==== Requirements

The following requirements are given by the lab instructions:

* Essentially, the lattice gas consists of a hexagonal array (the lattice), and a set of barriers.
* Each cell in the lattice is connected to 6 neighbors (see the illustrations in the report refereced above).
* Time is discrete (i.e. t = 0, 1, 2, 3...).
* Each connection is boolean: Thus, at time t, there is either 0 or 1 particle (but never 2 or more particles) traveling from any given cell a to any cellb, if a and b are neighbors (there might also be a particle traveling from b to a; the two links are independent of each other).
* The state of the lattice at time = t can be calculated in two phases:
* Transport: First, the particles travel linearly. For example, if cells a, b and c are linear neighbors (e.g. c is to the right of b, and b is to the right of a), then a particle traveling from a to b at time = 1 will continue to travel to c at time = 2 (in the absence of collisions, see below).
* Scattering: Second, collisions with other particles and with the barriers are taken into account. Collisions between particles are non-deterministic: More than one outcome is (sometimes) possible, and the simulation chooses between these outcomes randomly. See page 14 (188) in [Has87] for a table of the possible collisions.
* Particles reflect off barriers.



=== Implementation

We will store the hexagons row-wise in an odd-q vertical layout:

image::assets/hexagon-layout.png[Hexagon grid]

Each row will be processed independendly from each other. Every cell is stored in a single byte with one bit for each direction. The first bit is the north-east direction, the second bit is the east direction, and so on. The last bit is unused. The first byte of each row is the north-east corner of the row. The first byte of the second row is the north-east corner of the second row, and so on. The last byte of each row is the south-east corner of the row. The last byte of the second row is the south-east corner of the second row, and so on. The last byte of the last row is the south-east corner of the last row.

// packetdiag
[packetdiag]
....
packetdiag {
  node_height = 72;
  node_width = 90;

  0: WEST;
  1: NORTH_WEST;
  2: NORTH_EAST;
  3: EAST;
  4: SOUTH_EAST;
  5: SOUTH_WEST;
  6-7: unused;
}
....

To calculate the next state of a cell, we need to look at all of its neighbouring cells and move the particles into the current cell. 

.Movement calculation
[source#movement_implementation.linenums,cpp]
----
*current = (west & 0b00000001)
         | (north_west & 0b00000010)
         | (north_east & 0b00000100)
         | (east & 0b00001000)
         | (south_east & 0b00010000)
         | (south_west & 0b00100000);
----


As the GSI has machines that support AVX512 SIMD instructions we want to write code that can be autovectorized by the rust compiler. We used https://godbolt.org to view the generated machine code and make sure that our implementation supports AVX512. 

The movement step will be vectorized to something like this:

.Movement calculation (shortened)
[source#movement_assembly.linenums,assembly]
----
.LCPI0_28:
        .byte   2
.LCPI0_29:
        .byte   1
.LCPI0_30:
        .byte   4
.LCPI0_31:
        .byte   8
.LCPI0_32:
        .byte   16
.LCPI0_33:
        .byte   32

calculate_movement:
        ...
        vpbroadcastb    zmm0, byte ptr [rip + .LCPI0_28]
        vpbroadcastb    zmm1, byte ptr [rip + .LCPI0_29]
        vpbroadcastb    zmm2, byte ptr [rip + .LCPI0_30]
        vpbroadcastb    zmm3, byte ptr [rip + .LCPI0_31]
        vpbroadcastb    zmm4, byte ptr [rip + .LCPI0_32]
        vpbroadcastb    zmm5, byte ptr [rip + .LCPI0_33]
.LBB0_1:
        vpandq          zmm14, zmm0, zmmword ptr [rdi + rax]
        vpternlogq      zmm14, zmm1, zmmword ptr [rsi + rax], 248
        vpternlogq      zmm14, zmm2, zmmword ptr [rdi + rax + 1], 248
        vpternlogq      zmm14, zmm3, zmmword ptr [rsi + rax + 2], 248
        vpternlogq      zmm14, zmm4, zmmword ptr [rdx + rax + 1], 248
        vpternlogq      zmm14, zmm5, zmmword ptr [rdx + rax], 248
        add     rax, 64
        cmp     rax, 4928
        jne     .LBB0_1
        ...
        ret
----

After the movement step, we process the collisions. The rules for the collisions are quite simple:

* If a cell only has two particles traveling in opposite directions, they will rotate by either 60Â°.
* If a cell has three particles traveling in opposite directions, they will rotate by 60Â°.
* If a cell has exactly four particles with two pairs of opposite directions, they will rotate by 60Â°.

This can be implemented with the following match statement:

.Collision calculation using match
[source#collision_implementation.linenums,rust]
----
pub fn process_collision(number: u8) -> u8 {
    match number {
        // Two opposing particles
        0b00001001 => 0b00100100,
        0b00010010 => 0b00001001,
        0b00100100 => 0b00010010,
          
        // Three particles
        0b00101010 => 0b00010101,
        0b00010101 => 0b00101010,

        // Four particles with opposing holes
        0b00110110 => 0b00011011,
        0b00011011 => 0b00101101,
        0b00101101 => 0b00011011,

        // Everything else
        x => x,
    }
}
----

However when we look at the generated assembly, we see that the match statement is not autovectorized. Instead the compiler generates a jump table and uses a scalar comparison to find the correct branch. This is not ideal, as we want to use the AVX512 instructions to process multiple cells at once. We found that the most reliable way to get autovectoriztation is to use multiple simple if statements that can be implemented with a

.Collision calculation using if statements
[source#collision_implementation_if.linenums,rust]
----
    if *result == 0b00101010 {
        *result = 0b00010101;
    }
    if *result == 0b00010101 {
        *result = 0b00101010;
    }
    if *result == 0b00100100 {
        *result = 0b00010010;
    }
    if *result == 0b00010010 {
        *result = 0b00001001;
    }
    if *result == 0b00001001  {
        *result = 0b00100100;
    }
    if *result == 0b00110110  {
        *result = 0b00011011;
    }
    if *result == 0b00101101 {
        *result = 0b00110110;
    }
    if *result == 0b00011011 {
        *result = 0b00101101;
    }
----

results in the following assembly:

.Movement calculation (shortened)
[source#movement_assembly.linenums,assembly]
----
.LCPI0_34:
        .byte   42
.LCPI0_35:
        .byte   21
.LCPI0_36:
        .byte   36
.LCPI0_37:
        .byte   18
.LCPI0_38:
        .byte   9
.LCPI0_39:
        .byte   54
.LCPI0_40:
        .byte   27
.LCPI0_41:
        .byte   45

process_collisions:
        ...
        vpbroadcastb    zmm6, byte ptr [rip + .LCPI0_34]
        vpbroadcastb    zmm7, byte ptr [rip + .LCPI0_35]
        vpbroadcastb    zmm8, byte ptr [rip + .LCPI0_36]
        vpbroadcastb    zmm9, byte ptr [rip + .LCPI0_37]
        vpbroadcastb    zmm10, byte ptr [rip + .LCPI0_38]
        vpbroadcastb    zmm11, byte ptr [rip + .LCPI0_39]
        vpbroadcastb    zmm12, byte ptr [rip + .LCPI0_40]
        vpbroadcastb    zmm13, byte ptr [rip + .LCPI0_41]
        ...
.LBB0_1:
        ... ; zmm14 contains 64 cells
        vpcmpeqb        k1, zmm14, zmm6
        vmovdqu8        zmm14 {k1}, zmm7
        vpcmpeqb        k1, zmm14, zmm7
        vmovdqu8        zmm14 {k1}, zmm6
        vpcmpeqb        k1, zmm14, zmm8
        vmovdqu8        zmm14 {k1}, zmm9
        vpcmpeqb        k1, zmm14, zmm9
        vmovdqu8        zmm14 {k1}, zmm10
        vpcmpeqb        k1, zmm14, zmm10
        vmovdqu8        zmm14 {k1}, zmm8
        vpcmpeqb        k1, zmm14, zmm11
        vmovdqu8        zmm14 {k1}, zmm12
        vpcmpeqb        k1, zmm14, zmm13
        vmovdqu8        zmm14 {k1}, zmm11
        vpcmpeqb        k1, zmm14, zmm12
        vmovdqu8        zmm14 {k1}, zmm13
        add     rax, 64
        cmp     rax, 4928
        jne     .LBB0_1
        ...
        ret
----

However this implementation still has a few logic bugs. For example the case with three particles will be rotated twice, as the first if statement will match and then the second if statement will match. We tried multiple ways to fix it, the ones below all broke auto vectorization:
* Using a match statement with a guard clause
* Using if else statements

We ended up assigning the value of the cell to a temporary variable and using that as the basis for comparison. This way the result of the previous if statement is not used as the input for the next if statement. For some reason auto vectorization breaks when we dont reassing the value to a temporary variable.

[source#collision_implementation_if.linenums,rust]
----
*result = temp;
if temp == 0b00100100 {
    *result = 0b00010010;
}
if temp == 0b00010010 {
    *result = 0b00001001;
}
if temp == 0b00001001  {
    *result = 0b00100100;
}
if temp == 0b00101010 {
    *result = 0b00010101;
}
if temp == 0b00010101 {
    *result = 0b00101010;
}
temp = *result;
if temp == 0b00110110  {
    *result = 0b00011011;
}
if temp == 0b00101101 {
    *result = 0b00110110;
}
if temp == 0b00011011 {
    *result = 0b00101101;
}
----

This implementation always turns to the right on collisions. Implementing true randomness without sacrificing performance is not trivial. As a workaround, we adjusted our algorithm to

Duration per round with real random: 0.5
Duration per round without collisions: 0.15
Duration with fast collisions: 0.15
Duration with real collisions during movement: 1.2

=== Results

Our results will focus on the following three questions from the exercise sheet:

* Measure the performance in terms of the number of cells X time steps which can be calculated per minute.
* Determine the empirical relationship between the number of processors and the performance, and between the size of the lattice and the performance.
* What is the largest lattice which you can simulate?


=== Measurements

// We did not have time to measure performance at the Virgo cluster. We did measure the performance on our local machines. We used a 10GB file with generated with gensort. The single node implemetation is the non-mpi implementation. The measurements for more than one node are the MPI implementation.
// <<sorting-data>> show that our MPI implementation is always slower than the single threaded implementation. This is probably due to the overhead of sending the data over the network. We could probably optimize our implementation to be better, but we did not have time to do so.

We measured performance for datasets sized between 2^10 to 2^22 bytes on the Virgo cluster. These datasets were sorted by 4 algorithms:
`radix-sort`:: A single threaded radix sort implementation.
`unstable_sort`:: The rust standard librarys `sort_unstable()` https://doc.rust-lang.org/std/primitive.slice.html#method.sort_unstable function which uses pattern-defeating quicksort. 
`mpi-single`:: Our MPI implementation with a single thread per node.
`mpi-multi`:: Our MPI implementation with 2-4 threades per node, so reading, transmitting, sorting, and writing can happen in parallel.

We tested the MPI implementations with 1 to 16 nodes and 1 to 16 tasks per node (but at least 2 tasks). The two non-MPI implementations were tested with one node/one task. First the input file is copied to the `/tmp` directory on the manager node. The sorting algrithm then uses that file as input to avoid lustre bottlenecks. The output file is also written to the `/tmp` directory on the manager node. The measurement does not include coping the input file to the `/tmp` directory or copying the output file from the `/tmp` directory. The measurement does include reading and writing of the files from the `/tmp` directory. All measurements were made from inside our application. We also measured individual times for reading, transmitting, sorting, and writing to indentify bottlenecks. We tried to measure each implementation for each applicable combination of nodes and tasks per node 16 times. We bundled four measurements for the same configuration into one slurm batch.  In reality we got fewer measurements, as they sometimes fail. 

=== Non distributed performance

We want to select a non-distributed implementation to use as a baseline when comparing the distributed implementations. <<non-distributed-performance-total>> shows the total runtime of the two tested non-distributed implementations for different dataset sizes. It shows that the `sort-unstable` implementation is faster than the `radix-sort` implementation for smaller datasets. It's measurements are also more predictable, as the curve does not have a weird bump at 2^18 bytes. <<non-distributed-performance-per-step>> shows that the bump is probably not a measurement error, as the difference is in the sorting step and not while reading the input or writing the result.

.Total runtime comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-total
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-total.vl.json
include::scripts/vega-chart.adoc[]

.Runtime breakdown comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-per-step
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-per-step.vl.json
include::scripts/vega-chart.adoc[]

.Relative step comparison of the non-distributed implementations
:chart-id: id=non-distributed-performance-percentage
:vega-lite-filename: processed-assets/sorting-non-distributed-performance-percentage.vl.json
include::scripts/vega-chart.adoc[]

We will use the `sort-unstable` implementation as a baseline for the distributed implementations, because it is faster and has a more predictable runtime.

=== Distributed performance

<<mpi-single-performance-one-node-speedup>> shows the relative speedup of the distributed implementation with different numbers of tasks compared to `sort-unstable`. Each line represents a implementation running with a given number of ranks. Every rank runs on the same machine.

.Speedup compared to non-distributed sorting
:chart-id: id=mpi-single-performance-one-node-speedup
:vega-lite-filename: processed-assets/sorting-mpi-single-performance-one-node-speedup.vl.json
include::scripts/vega-chart.adoc[]

.Efficiency compared to non-distributed sorting
:chart-id: id=mpi-single-performance-one-node-efficiency
:vega-lite-filename: processed-assets/sorting-mpi-single-performance-one-node-efficiency.vl.json
include::scripts/vega-chart.adoc[]

It shows that the distributed implementation is slower than the non-distributed implementation for small problems (n <= 2^20 entries). The best relative speedup is achieved for problems with a size of 2^24 , after that the relative speedup gets lower again.

When only two tasks are used, there is no speedup. With 2^24 entries the speedup is exactly 1, which means that the distributed implementation is as fast as the non-distributed implementation. I would have expected it to be lower, as we only have one worker that is actually sorting in that case. The advantage we get by bucketing the data and only sorting small chunks, seems to be equal to the overhead of distributing it into buckets and sending the data over the network.
// Wrong... It is only half that..

When using 4 or 8 tasks the speedup is 2 for 2^24 entries. For 16, 32, and 64 tasks the speedup is 3 for 2^24 entries. It looks like the maximum speedup is achieved when using 16 tasks, after that we get diminishing returns.


.Duration of each step when sorting 16M numbers
:chart-id: id=one-node-16M-steps
:vega-lite-filename: processed-assets/sorting-one-node-16M-steps.vl.json
include::scripts/vega-chart.adoc[]


=== Learnings

Rust MPI is ok, but not great. It is not very ergonomic as it mostly just wraps C calls. The documentation is acceptable. One of the major pitfalls we encountered was with the `receive_vec` function which is a wrapper around `MPI_recv`. It puts the data directly in a heap allocated vector. It is quite slow for large buffers, as it does not know the size of the data it is receiving in advance, so it has to grow the Vec repeatedly. We ended up using `receive_into` a buffer instead, which is a bit more verbose, but much faster.

We also learned that MPI has a limit of 2GB per buffer, which can be problematic when working with large datasets, like 100GB of sorting data.

The posix fadvise API is cool and can be used to tell the OS that we are going to read a file sequentially and only once. This can improve performance by a tiny bit, as the kernel can prefetch more the data. Memadvise is probably also cool, but we did not have time to try it out.

Lustre can be quite slow, probably due to congestion.

=== Benchmarking protocol

We want to benchmark our algorithm for the 1G entries dataset. Our benchmark has to read the data from the /tmp partition from a single node and write the result back to the /tmp partition on a single node. We do this, because we want to benchmark our algorithm (and the performance of the nodes and the networking performance). Sorting should happen in a distributed fashion.

First, we will copy the data to the tmp partition of the root node to avoid the pitfalls of using lustre. THen...

.Our MPI implementation
:chart-id: id=sorting-data
:vega-lite-filename: processed-assets/sorting-data.vl.json
include::scripts/vega-chart.adoc[]

[glossary]
== List of abbreviations
// Abbreviations from here will automatically be linked to the document

// Abbreviations in random order and links to read more about them
[glossary]
[[LGCA]]LGCA:: Lattice gas cellular automaton link:pass:[https://en.wikipedia.org/wiki/Lattice_gas_automaton][ðŸ”—^]


include::scripts/trailing-scripts.adoc[]